{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cifar 10 데이터 로드\n",
    "(x_train , y_train),(x_test, y_test)= cifar10.load_data()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## cifar 10 dataset 시각화\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "for i in range(36):\n",
    "  ax = fig.add_subplot(3,12, i+1, xticks=[], yticks=[])\n",
    "  ax.imshow(np.squeeze(x_train[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 정규화\n",
    "x_train = x_train.astype('float32')/ 255\n",
    "x_test = x_test.astype('float32')/ 255\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "## one-hot encoding\n",
    "## cifar10 / class = 10 / 라벨이 10개\n",
    "num_classes =  len(np.unique(y_train))\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "## valid 층을 train에서 따로 떼어 놓음 \n",
    "(x_train, x_valid) = x_train[5000:], x_train[:5000]\n",
    "(y_train, y_valid) = y_train[5000:], y_train[:5000]\n",
    "\n",
    "print(x_train.shape,  x_valid.shape, x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout ,BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "## sequential 모델 사용\n",
    "model= Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size = 2, padding='same',activation= 'relu' , input_shape=(32,32,3)))\n",
    "## 특징을 포함하되 사이즈를 반으로 \n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size = 2, padding='same' ,activation= 'relu' ))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size = 2, padding='same',activation= 'relu' ))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10 ,activation='softmax' ))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "## 학습전 컴파일 옵션 // metrics=['accuracy'] 는 분류 대상이 3개 이상일때 \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## val_loss가 더이상 떨어지지 않는 것이 10번  쌓이면 학습 중지\n",
    "earlyStopping = EarlyStopping (monitor='val_loss', min_delta = 0, patience=10)\n",
    "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, save_best_only=True)\n",
    "hist = model.fit(x_train,y_train, batch_size=32, epochs=100, validation_data=(x_valid,y_valid), callbacks=[checkpointer, earlyStopping],verbose=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2= Sequential()\n",
    "model2.add(Conv2D(filters=32, kernel_size = 2, padding='same',activation= 'relu' , kernel_initializer='he_normal',input_shape=(32,32,3)))\n",
    "## 배치 사이즈 정규화 실행\n",
    "model2.add(BatchNormalization())\n",
    "## 20프로의 뉴런 비활성화 --> 과적합 방지\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Conv2D(filters=64, kernel_size = 3, padding='same' ,activation= 'relu' ,kernel_initializer='he_normal'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPooling2D(pool_size=2))\n",
    "model2.add(Conv2D(filters=128, kernel_size = 2, padding='same' ,activation= 'relu',kernel_initializer='he_normal' ))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPooling2D(pool_size=2))\n",
    "model2.add(Conv2D(filters=64, kernel_size = 2, padding='same' ,activation= 'relu',kernel_initializer='he_normal' ))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPooling2D(pool_size=2))\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(256, activation='relu'))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Dense(10 ,activation='softmax' ,kernel_initializer='glorot_uniform'))\n",
    "\n",
    "## 에폭마다 wb를 저장하기 위해\n",
    "model2.load_weights('/content/model.weights.best.hdf5')\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model2.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "\n",
    "# compile model\n",
    "opt = SGD(lr=0.001, momentum=0.9)\n",
    "model2.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "## callbacks=[checkpointer] 를 넣어서 에폭이 돌때마다 WB 저장\n",
    "hist = model2.fit(x_train,y_train, batch_size=64, epochs=100, validation_data=(x_valid,y_valid), callbacks=[checkpointer],verbose=2, shuffle=True)\n",
    "\n",
    "print(hist.history['loss'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
