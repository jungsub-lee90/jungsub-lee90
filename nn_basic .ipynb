{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "from util.layers import *\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet :\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        self.params={}\n",
    "        ## 인풋 -> 은닉 : 초기값 설정\n",
    "        self.params['W1']= weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1']= np.zeros(hidden_size)\n",
    "        ## 은닉 -> 아웃풋 : 초기값 설정\n",
    "        self.params['W2']= weight_init_std  * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2']=np.zeros(output_size)\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        ## layers dict에 계층의 wx + b 와 활성함수(relu)를 쌍으로 담는다.\n",
    "        self.layers['Affine1']= Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['relu1'] = Relu()\n",
    "        self.layers['Affine2']= Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        ## 마지막 아웃풋 layer에서 사용할 함수 softmax(예측값 3개 이상시 사용) and loss(손실 값 확인 후 감소 위한)\n",
    "        self.last_layers = SoftmaxWithLoss()        \n",
    "    ## predict = forward\n",
    "    def predict(self , x ):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x,t):\n",
    "        y= self.predict(x)\n",
    "        \n",
    "        ## SoftmaxWithLoss() 클래스의 forward 함수 사용\n",
    "        return self.last_layers.forward(y,t)\n",
    "\n",
    "    def accuracy (self, x,t):\n",
    "        y= self.predict(x)\n",
    "        y= np.argmax(y,axis=1)\n",
    "        t= np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y==t)/ x.shape[0]\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x,t):\n",
    "        loss_W = lambda _ : self.loss(x,t)\n",
    "        \n",
    "        grads={}\n",
    "        grads['W1']= numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1']= numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2']= numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2']= numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x,t):\n",
    "        ## dout은 backward 할때 미분계수\n",
    "        self.loss(x,t)\n",
    "        dout =1\n",
    "        dout = self.last_layers.backward(dout)\n",
    "        \n",
    "        ## backward에서는 layers에 과정을 거꾸로 해야 하기 때문에 .reverse()\n",
    "        layers= list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        grads={}\n",
    "        grads['W1']= self.layers['Affine1'].dW\n",
    "        grads['b1']= self.layers['Affine1'].db\n",
    "        grads['W2']= self.layers['Affine2'].dW\n",
    "        grads['b2']= self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터셋 불러오기\n",
    "(x_train , t_train),(x_test , t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "## nn 파라미터 세팅\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 배치 사이즈 설정\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "## 미분을 구하면서 하는 역전파 과정\n",
    "grad_numerical =network.numerical_gradient(x_batch,t_batch)\n",
    "## 순전파의 미분계수를 이용해 단순 계산만 하는 역전파\n",
    "grad_backpropa = network.gradient(x_batch, t_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 (784, 50) (784, 50)\n",
      "W1:5.9939684989628335e-09\n",
      "b1 (50,) (50,)\n",
      "b1:4.703852263208085e-08\n",
      "W2 (50, 10) (50, 10)\n",
      "W2:8.616688927349332e-08\n",
      "b2 (10,) (10,)\n",
      "b2:1.8002014676360422e-06\n"
     ]
    }
   ],
   "source": [
    "for key in grad_numerical.keys():\n",
    "    \n",
    "    print(key, grad_backpropa[key].shape, grad_numerical[key].shape)\n",
    "    ## 두개의 알고리즘의 차이를 확인하고자 - 연산\n",
    "    diff = np.average(np.abs(grad_backpropa[key] - grad_numerical[key]))\n",
    "    print(key + ':' + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 216/10000 [00:00<00:15, 650.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.11925/ test acc 0.1227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 717/10000 [00:01<00:15, 581.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.9246333333333333/ test acc 0.9243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 1363/10000 [00:02<00:13, 630.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.9504/ test acc 0.9502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1905/10000 [00:02<00:13, 612.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.95835/ test acc 0.9531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2542/10000 [00:03<00:11, 631.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.97275/ test acc 0.9675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 3080/10000 [00:04<00:11, 617.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.974/ test acc 0.9677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 3727/10000 [00:05<00:12, 504.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.9741833333333333/ test acc 0.9677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 4273/10000 [00:06<00:10, 554.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.9783666666666667/ test acc 0.9691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 4952/10000 [00:07<00:09, 544.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.9822/ test acc 0.9706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5605/10000 [00:08<00:06, 697.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.9836166666666667/ test acc 0.9735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 6154/10000 [00:09<00:06, 603.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.9847666666666667/ test acc 0.974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6673/10000 [00:10<00:06, 517.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.9839/ test acc 0.9715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 7327/10000 [00:11<00:05, 511.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.9849833333333333/ test acc 0.9709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 7921/10000 [00:12<00:03, 520.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.9884166666666667/ test acc 0.9742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 8521/10000 [00:13<00:02, 556.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.9890333333333333/ test acc 0.9751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 9170/10000 [00:14<00:01, 640.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.9868833333333333/ test acc 0.973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 9717/10000 [00:15<00:00, 628.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.9915333333333334/ test acc 0.9742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:15<00:00, 649.09it/s]\n"
     ]
    }
   ],
   "source": [
    "(x_train , t_train),(x_test , t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "## setting\n",
    "train_loss_list=[]\n",
    "## 정확도 리스트\n",
    "train_acc_list=[]\n",
    "test_acc_list=[]\n",
    "iter_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.3\n",
    "\n",
    "#\n",
    "iter_per_epoch = train_size / batch_size\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "## 만번 수행\n",
    "for i in tqdm(range(iter_num)):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    ## 랜덤 행렬 100개 넣는다.\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    ## 100 개의 이미지씩 기울기를 구한다.\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        ## 네트워크에 있는 w와 b에 grad의 미분값을 러닝레이트에 곱한 값을 빼준다. \n",
    "        network.params[key]-= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "\n",
    "    ## epoach 돌때마다 정확도 출력\n",
    "    if i % iter_per_epoch ==0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc =  network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print (f'train acc : {str(train_acc)}/ test acc {str(test_acc)}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49e04899c6e094c3b61dd8509fde2649571811ca1eef12c1c216e72ad0930740"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
