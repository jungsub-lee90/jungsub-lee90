{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "## cifar 10 데이터 로드\n",
    "(x_train , y_train),(x_test, y_test)= cifar10.load_data()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## cifar 10 dataset 시각화\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "for i in range(36):\n",
    "  ax = fig.add_subplot(3,12, i+1, xticks=[], yticks=[])\n",
    "  ax.imshow(np.squeeze(x_train[i]))\n",
    "\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "(x_train, x_valid) = x_train[5000:], x_train[:5000]\n",
    "(y_train, y_valid) = y_train[5000:], y_train[:5000]\n",
    "\n",
    "## normailze 대신 standarized 함 normalize = x_train / 255\n",
    "## 각각의 rgb를 합쳐준다\n",
    "mean = np.mean(x_train, axis=(0,1,2,3))\n",
    "std = np.std(x_train, axis=(0,1,2,3))\n",
    "## 데이터 정규화 (픽셀값 - 픽셀값의 평균) / 표준편차\n",
    "x_train =(x_train-mean) / (std+1e-7)\n",
    "x_valid =(x_valid-mean) / (std+1e-7)\n",
    "x_test =(x_test-mean) / (std+1e-7)\n",
    "\n",
    "## one-hot encoding\n",
    "## cifar10 / class = 10 / 라벨이 10개\n",
    "num_classes =  len(np.unique(y_train))\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_valid = to_categorical(y_valid , num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(x_train.shape,  x_valid.shape, x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout ,BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "## 데이터 강화 -> 과적합 방지 및 모델 쫀쫀\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range= 0.1,\n",
    "    height_shift_range= 0.1,\n",
    "    horizontal_flip= True,\n",
    "    vertical_flip=False\n",
    ")\n",
    "## 변환 데이터 생성\n",
    "datagen.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train.shape[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 앞에서 스탠달다이즈 했기때문에 batch_noraml에서 non-trainable이 걸린다.\n",
    "## kernel_regularizer -> 과적합 방지\n",
    "from keras import regularizers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters =32, kernel_size=3 , padding='same', kernel_regularizer=regularizers.l2(1e-4), input_shape=x_train.shape[1:]  , activation='relu'))\n",
    "## 이미 추출된 특징을 다시 정규화\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters =32, kernel_size=3 , padding='same', kernel_regularizer=regularizers.l2(1e-4)  , activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "## 이미지 특징들의 중요요소를 압축 -> 파라미터수 조절 -> 학습량 다운\n",
    "model.add(MaxPooling2D(pool_size = 2))\n",
    "## 뉴런들을 비활성화해서 가중치가 고루 분배되게 만듦 -> 과적합 방지\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(filters =64, kernel_size=3 , padding='same', kernel_regularizer=regularizers.l2(1e-4)  , activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters =64, kernel_size=3 , padding='same', kernel_regularizer=regularizers.l2(1e-4)  , activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size = 2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv2D(filters =128, kernel_size=3 , padding='same', kernel_regularizer=regularizers.l2(1e-4)  , activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters =128, kernel_size=3 , padding='same', kernel_regularizer=regularizers.l2(1e-4)  , activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size = 2))\n",
    "### 풀링을 거치면 입력 32*32*3 -> 4*4*128로 변경\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "from keras.mixed_precision.loss_scale_optimizer import optimizers\n",
    "from keras.callbacks import ModelCheckpoint , EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "## BGD 데이터 전체가 1 epoch으로 --> 메모리 많이 씀\n",
    "## SGD 데이터를 한개가 1 epoch  --> 느림 \n",
    "## 미니배치 (BGD와 SGD의 중간)\n",
    "batch_size = 128\n",
    "epochs = 125\n",
    "\n",
    "## load_weight으로 불러오기 위한 체크포인트, 에폭마다 모델저장\n",
    "checkpoint = ModelCheckpoint('model.125epochs.hdf5', verbose=1 , save_best_only=True)\n",
    "earlyStopping = EarlyStopping (monitor='val_loss', min_delta = 0, patience=10)\n",
    "## learning rate / decay 감쇠율\n",
    "optimizer = Adam(lr=0.0001, decay= 1e-6)\n",
    "\n",
    "##categorical_crossentropy 정확도와 손실률은 꼭 정비례는 아니다.\n",
    "model.compile(loss='categorical_crossentropy', optimizer = optimizer, metrics=['accuracy'])\n",
    "\n",
    "## 강화데이터로 학습\n",
    "hist = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size), callbacks=[checkpoint], epochs= epochs, verbose=2, validation_data=(x_valid, y_valid))\n",
    "\n",
    "scores = model.evaluate(x_valid,y_valid, batch_size=128, verbose=1)\n",
    "\n",
    "print(f'loss : {scores[0]} , acc : {scores[1]}' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## history['loss'] 손실율 / history['acc'] 정확도\n",
    "plt.plot(hist.history['loss'], label='train')\n",
    "plt.plot(hist.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout ,BatchNormalization , AveragePooling2D\n",
    "model2 = Sequential()\n",
    "model2.add(Conv2D(6, kernel_size=5 , strides=1 ,activation='tanh', input_shape=(32,32,3), padding='same' ))\n",
    "model2.add(AveragePooling2D(pool_size=2, strides=2 ))\n",
    "model2.add(Conv2D(16, kernel_size=5 , strides=1 ,activation='tanh' ))\n",
    "model2.add(AveragePooling2D(pool_size=2, strides=2 ))\n",
    "model2.add(Conv2D(120, kernel_size=5 , strides=1 ,activation='tanh' ))\n",
    "model2.add(Flatten())\n",
    "#model2.add(Dense(120, activation='tanh'))\n",
    "model2.add(Dense(84, activation='tanh'))\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "model2.summary()\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.datasets import cifar10\n",
    "(x_train , y_train),(x_test, y_test)= cifar10.load_data()\n",
    "## valid 층을 train에서 따로 떼어 놓음 \n",
    "(x_train, x_valid) = x_train[5000:], x_train[:5000]\n",
    "(y_train, y_valid) = y_train[5000:], y_train[:5000]\n",
    "\n",
    "## 데이터 정규화\n",
    "x_train = x_train.astype('float32')/ 255\n",
    "x_test = x_test.astype('float32')/ 255\n",
    "x_valid=x_valid.astype('float32')/255\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "## one-hot encoding\n",
    "## cifar10 / class = 10 / 라벨이 10개\n",
    "num_classes =  len(np.unique(y_train))\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "y_valid = to_categorical(y_valid, num_classes)\n",
    "\n",
    "print(x_train.shape,  x_valid.shape, x_test.shape)\n",
    "\n",
    "## 에포크 수에 따라 lr 조절\n",
    "def schedule(epoch):\n",
    "  if epoch <=2 :\n",
    "    lr = 0.0005\n",
    "  elif epoch >2 and epoch <=5:\n",
    "    lr = 0.0002\n",
    "  elif epoch >5 and epoch <9 :\n",
    "    lr = 0.00005\n",
    "  else : \n",
    "    lr =0.00001\n",
    "  return lr \n",
    "\n",
    "## 스케줄러를 통한 lr 조절(선행 함수 필요)\n",
    "lr_scheduler = LearningRateScheduler(schedule)\n",
    "checkpoint = ModelCheckpoint('model.125epochs.hdf5', verbose=1 ,monitor= 'val_accuracy', save_best_only=True)\n",
    "callboacks= [checkpoint, lr_scheduler]\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', optimizer = 'sgd', metrics=['accuracy'])\n",
    "hist = model2.fit(x_train, y_train, batch_size=32, epochs=20, validation_data=(x_test, y_test), callbacks= callboacks, verbose=2 , shuffle=True)\n",
    "\n",
    "model2.load_weights('/content/model.relu.125epochs.hdf5')\n",
    "model2.evaluate(x_test,y_test, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "alexnet= Sequential()\n",
    "alexnet.add(Conv2D(64, kernel_size=7, activation='relu', input_shape=(32,32,3)))\n",
    "#alexnet.add(MaxPooling2D(pool_size=2, strides=2 ))\n",
    "alexnet.add(BatchNormalization())\n",
    "alexnet.add(Conv2D(192, kernel_size=5, activation='relu', padding='same', kernel_regularizer=l2(0.0005)))\n",
    "alexnet.add(MaxPooling2D(pool_size=3, strides=2 ))\n",
    "alexnet.add(BatchNormalization())\n",
    "alexnet.add(Conv2D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.0005)))\n",
    "alexnet.add(Conv2D(256, kernel_size=3, activation='relu' ,padding='same', kernel_regularizer=l2(0.0005)))\n",
    "alexnet.add(Conv2D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(0.0005)))\n",
    "alexnet.add(MaxPooling2D(pool_size=3, strides=2 ))\n",
    "\n",
    "alexnet.add(Flatten())\n",
    "alexnet.add(Dense(2048, activation='relu'))\n",
    "alexnet.add(Dropout(0.5))\n",
    "alexnet.add(Dense(2048, activation='relu'))\n",
    "alexnet.add(Dropout(0.5))\n",
    "alexnet.add(Dense(10, activation='softmax'))\n",
    "alexnet.summary()\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from keras.optimizers import SGD\n",
    "## 검증오차가 정체될 때마다 학습률 10프로 감소\n",
    "reduce_lr = ReduceLROnPlateau(monitor ='val_loss', factor=np.sqrt(0.1))\n",
    "optimizer = SGD(lr=0.01, momentum=0.9 )\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.datasets import cifar10\n",
    "(x_train , y_train),(x_test, y_test)= cifar10.load_data()\n",
    "## valid 층을 train에서 따로 떼어 놓음 \n",
    "(x_train, x_valid) = x_train[5000:], x_train[:5000]\n",
    "(y_train, y_valid) = y_train[5000:], y_train[:5000]\n",
    "\n",
    "## 데이터 정규화\n",
    "x_train = x_train.astype('float32')/ 255\n",
    "x_test = x_test.astype('float32')/ 255\n",
    "x_valid=x_valid.astype('float32')/255\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "## one-hot encoding\n",
    "## cifar10 / class = 10 / 라벨이 10개\n",
    "num_classes =  len(np.unique(y_train))\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "y_valid = to_categorical(y_valid, num_classes)\n",
    "\n",
    "print(x_train.shape,  x_valid.shape, x_test.shape)\n",
    "\n",
    "\n",
    "alexnet.compile(loss='categorical_crossentropy', optimizer = optimizer, metrics=['accuracy'])\n",
    "hist = alexnet.fit(x_train, y_train, batch_size=128, epochs=90, validation_data=(x_valid, y_valid), callbacks= [reduce_lr], verbose=2 , shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49e04899c6e094c3b61dd8509fde2649571811ca1eef12c1c216e72ad0930740"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
